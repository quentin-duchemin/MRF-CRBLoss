{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have saved a pickle file containing all the settings of the network that you want to train (see the corresponding tutorial), you are read to launch the training.\n",
    "\n",
    "To do so, you only need to launch the script `main_offline.py` specifying the name the settings file that you saved. More precisely, you just need to use the command line\n",
    "\n",
    "```bash\n",
    "\tpython main_offline.py --save_name **chosen_name**\n",
    "```\n",
    "\n",
    "Hence nothing special to report. That's why this tutorial is dedicated to developers (or curious readers) that want to have some comments on the way the script file `main_offline.py` is built. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Loading parameters from the setting file\n",
    "\n",
    "We start by loading the parameters for the training saved in the pickle file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "depth = '../'\n",
    "for i in range(5):\n",
    "    sys.path.append(depth)\n",
    "    depth += '../'\n",
    "\n",
    "from MRF.Offline.Network import *\n",
    "from MRF.Training_parameters import *\n",
    "from MRF.Offline.Data_class import *\n",
    "from MRF.Projection import *\n",
    "import pickle\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "\n",
    "def load_parser():\n",
    "    parser = argparse.ArgumentParser(description='Description of the training parameters.')\n",
    "    parser.add_argument('-f','--save_name', type=str)\n",
    "    # the projection\n",
    "    parser.add_argument('--project_lr_times', type=int, default=1)\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    with open('settings_files_offline/settings_'+args.save_name+'.pkl', 'rb') as f:\n",
    "        default_settings = pickle.load(f)\n",
    "        parser.add_argument('--model', type=str, default=default_settings['model'])\n",
    "        parser.add_argument('--optimizer', type=str, default=default_settings['optimizer'])\n",
    "        parser.add_argument('--lr', type=float, default=default_settings['lr'])\n",
    "        parser.add_argument('--noise_type', type=str, default=default_settings['noise_type'])\n",
    "        parser.add_argument('--noise_level', type=float, default=default_settings['noise_level'])\n",
    "        parser.add_argument('--normalization', type=str, default=default_settings['normalization'])\n",
    "        parser.add_argument('--namepca', type=str, default=default_settings['namepca'])\n",
    "        parser.add_argument('--batch_size', type=int, default=default_settings['batch_size'])\n",
    "        parser.add_argument('--start_by_projection', type=bool, default=default_settings['start_by_projection'])\n",
    "        parser.add_argument('--nb_epochs', type=int, default=default_settings['nb_epochs'])\n",
    "        parser.add_argument('--initialization', type=str, default=default_settings['initialization'])\n",
    "        parser.add_argument('--params', metavar='N', type=int, nargs='+', default=default_settings['params'])\n",
    "        parser.add_argument('--loss', type=str, default=default_settings['loss'])\n",
    "        parser.add_argument('--minPD', type=float, default=default_settings['minPD'])\n",
    "        parser.add_argument('--maxPD', type=float, default=default_settings['maxPD'])\n",
    "        parser.add_argument('--validation', type=bool, default=default_settings['validation'])\n",
    "        parser.add_argument('--complex', type=bool, default=default_settings['complex'])\n",
    "        parser.add_argument('--small_validation_size', type=int, default=default_settings['small_validation_size'])\n",
    "        parser.add_argument('--validation_size', type=int, default=default_settings['validation_size'])\n",
    "        parser.add_argument('--dimension_projection', type=float, default=default_settings['dimension_projection'])\n",
    "        parser.add_argument('--nb_files', type=int, default=default_settings['nb_files'])\t\t\n",
    "        parser.add_argument('--path_files', type=str, default=default_settings['path_files'])\n",
    "        return parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Launching the training\n",
    "\n",
    "One parameters are loaded, we define all the class instances required to train a network.\n",
    "\n",
    "- An instance of the class `Training_parameters` that contains the value of all the hyperparameters useful for training (number of epochs, the batch size, ...)\n",
    "\n",
    "- An instance of the class `Data_class` that contains all the parameters related to data pre-processing (minimum and maximum values for the proton density scaling, the number of files saved for fingerprints, the noise level to apply, ...)\n",
    "\n",
    "- An instance of the class `Projection` that defines the way the network architecture will deal with the input signal. Does the network starts by projecting the signal ? Is the first layer fixed (and not learned)? What is the dimension of the projection subspace? How do we normalize the fingerprints? ...\n",
    "\n",
    "- An instance of the class `Network` that is the main class of our code. It takes as arguments the previously defined instances of classes, the name of the loss function to use and the value of the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    parser = load_parser()\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    training_parameters = Training_parameters(args.batch_size, 1, args.nb_epochs, args.params, args.normalization, args.complex)\n",
    "    \n",
    "    CRBrequired = False\n",
    "    for para in args.params:\n",
    "        CRBrequired = CRBrequired or Loss.CRBrequired(args.loss[para])\n",
    "    data_class = Data_class(training_parameters, args.noise_type, args.noise_level, args.minPD, args.maxPD, args.nb_files, args.path_files, CRBrequired = CRBrequired)\n",
    "    if args.start_by_projection:\n",
    "        projection = Projection(args.start_by_projection, args.dimension_projection, args.initialization, args.normalization, args.namepca, args.complex)\n",
    "    else:\n",
    "        projection = None\n",
    "\n",
    "    validation_settings = {'validation': args.validation,'small_validation_size': args.small_validation_size, 'validation_size': args.validation_size}\n",
    "    network = Network(args.model, args.loss, training_parameters, args.save_name, data_class, validation_settings, projection=projection)\n",
    "    network.train(lr = args.lr, nameoptimizer = args.optimizer, projection_lr_times=args.project_lr_times)\n",
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
